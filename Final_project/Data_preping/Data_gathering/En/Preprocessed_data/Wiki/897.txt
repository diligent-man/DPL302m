general purpose computing on graphics processing units gpgpu or less often gpgp is the use of a graphics processing unit gpu which typically handles computation only for computer graphics to perform computation in applications traditionally handled by the central processing unit cpu
the use of multiple video cards in one computer or large numbers of graphics chips further parallelizes the already parallel nature of graphics processing
essentially a gpgpu pipeline is a kind of parallel processing between one or more gpus and cpus that analyzes data as if it were in image or other graphic form
while gpus operate at lower frequencies they typically have many times the number of cores
thus gpus can process far more pictures and graphical data per second than a traditional cpu
migrating data into graphical form and then using the gpu to scan and analyze it can create a large speedup
gpgpu pipelines were developed at the beginning of the st century for graphics processing e g
for better shaders
these pipelines were found to fit scientific computing needs well and have since been developed in this direction
in principle any arbitrary boolean function including those of addition multiplication and other mathematical functions can be built up from a functionally complete set of logic operators
in conway s game of life became one of the first examples of general purpose computing using an early stream processor called a blitter to invoke a special sequence of logical operations on bit vectors
general purpose computing on gpus became more practical and popular after about with the advent of both programmable shaders and floating point support on graphics processors
notably problems involving matrices and or vectors especially two three or four dimensional vectors were easy to translate to a gpu which acts with native speed and support on those types
a significant milestone for gpgpu was the year when two research groups independently discovered gpu based approaches for the solution of general linear algebra problems on gpus that ran faster than on cpus
these early efforts to use gpus as general purpose processors required reformulating computational problems in terms of graphics primitives as supported by the two major apis for graphics processors opengl and directx
this cumbersome translation was obviated by the advent of general purpose programming languages and apis such as sh rapidmind brook and accelerator
these were followed by nvidia s cuda which allowed programmers to ignore the underlying graphical concepts in favor of more common high performance computing concepts
newer hardware vendor independent offerings include microsoft s directcompute and apple khronos group s opencl
this means that modern gpgpu pipelines can leverage the speed of a gpu without requiring full and explicit conversion of the data to a graphical form
mark harris the founder of gpgpu org coined the term gpgpu
any language that allows the code running on the cpu to poll a gpu shader for return values can create a gpgpu framework
programming standards for parallel computing include opencl vendor independent openacc openmp and openhmpp
as of opencl is the dominant open general purpose gpu computing language and is an open standard defined by the khronos group
opencl provides a cross platform gpgpu platform that additionally supports data parallel compute on cpus
opencl is actively supported on intel amd nvidia and arm platforms
the khronos group has also standardised and implemented sycl a higher level programming model for opencl as a single source domain specific embedded language based on pure c
the dominant proprietary framework is nvidia cuda
nvidia launched cuda in a software development kit sdk and application programming interface api that allows using the programming language c to code algorithms for execution on geforce series and later gpus
rocm launched in is amd s open source response to cuda
it is as of on par with cuda with regards to features and still lacking in consumer support
the xcelerit sdk created by xcelerit is designed to accelerate large existing c or c code bases on gpus with minimal effort
it provides a simplified programming model automates parallelisation manages devices and memory and compiles to cuda binaries
additionally multi core cpus and other accelerators can be targeted from the same source code
openvidia was developed at university of toronto between in collaboration with nvidia
altimesh hybridizer created by altimesh compiles common intermediate language to cuda binaries
it supports generics and virtual functions
debugging and profiling is integrated with visual studio and nsight
it is available as a visual studio extension on visual studio marketplace
microsoft introduced the directcompute gpu computing api released with the directx api
alea gpu created by quantalea introduces native gpu computing capabilities for the microsoft net language f and c
alea gpu also provides a simplified gpu programming model based on gpu parallel for and parallel aggregate using delegates and automatic memory management
matlab supports gpgpu acceleration using the parallel computing toolbox and matlab distributed computing server and third party packages like jacket
gpgpu processing is also used to simulate newtonian physics by physics engines and commercial implementations include havok physics fx and physx both of which are typically used for computer and video games
c accelerated massive parallelism c amp is a library that accelerates execution of c code by exploiting the data parallel hardware on gpus
due to a trend of increasing power of mobile gpus general purpose programming became available also on the mobile devices running major mobile operating systems
google android enabled running renderscript code on the mobile device gpu
apple introduced the proprietary metal api for ios applications able to execute arbitrary code through apple s gpu compute shaders
computer video cards are produced by various vendors such as nvidia amd
cards from such vendors differ on implementing data format support such as integer and floating point formats bit and bit
microsoft introduced a shader model standard to help rank the various features of graphic cards into a simple shader model version number etc
pre directx video cards only supported paletted or integer color types
various formats are available each containing a red element a green element and a blue element
sometimes another alpha value is added to be used for transparency
common formats are
bits per pixel sometimes palette mode where each value is an index in a table with the real color value specified in one of the other formats
sometimes three bits for red three bits for green and two bits for blue
bits per pixel usually the bits are allocated as five bits for red six bits for green and five bits for blue
bits per pixel there are eight bits for each of red green and blue
bits per pixel there are eight bits for each of red green blue and alpha for early fixed function or limited programmability graphics i e up to and including directx compliant gpus this was sufficient because this is also the representation used in displays
it is important to note that this representation does have certain limitations
given sufficient graphics processing power even graphics programmers would like to use better formats such as floating point data formats to obtain effects such as high dynamic range imaging
many gpgpu applications require floating point accuracy which came with video cards conforming to the directx specification
directx shader model x suggested the support of two precision types full and partial precision
full precision support could either be fp or fp floating point or bit per component or greater while partial precision was fp
ati s radeon r series of gpus supported fp precision only in the programmable fragment pipeline although fp was supported in the vertex processors while nvidia s nv series supported both fp and fp other vendors such as s graphics and xgi supported a mixture of formats up to fp
the implementations of floating point on nvidia gpus are mostly ieee compliant however this is not true across all vendors
this has implications for correctness which are considered important to some scientific applications
while bit floating point values double precision float are commonly available on cpus these are not universally supported on gpus
some gpu architectures sacrifice ieee compliance while others lack double precision
efforts have occurred to emulate double precision floating point values on gpus however the speed tradeoff negates any benefit to offloading the computing onto the gpu in the first place
most operations on the gpu operate in a vectorized fashion one operation can be performed on up to four values at once
for example if one color r g b is to be modulated by another color r g b the gpu can produce the resulting color r r g g b b in one operation
this functionality is useful in graphics because almost every basic data type is a vector either or dimensional
examples include vertices colors normal vectors and texture coordinates
many other applications can put this to good use and because of their higher performance vector instructions termed single instruction multiple data simd have long been available on cpus
originally data was simply passed one way from a central processing unit cpu to a graphics processing unit gpu then to a display device
as time progressed however it became valuable for gpus to store at first simple then complex structures of data to be passed back to the cpu that analyzed an image or a set of scientific data represented as a d or d format that a video card can understand
because the gpu has access to every draw operation it can analyze data in these forms quickly whereas a cpu must poll every pixel or data element much more slowly as the speed of access between a cpu and its larger pool of random access memory or in an even worse case a hard drive is slower than gpus and video cards which typically contain smaller amounts of more expensive memory that is much faster to access
transferring the portion of the data set to be actively analyzed to that gpu memory in the form of textures or other easily readable gpu forms results in speed increase
the distinguishing feature of a gpgpu design is the ability to transfer information bidirectionally back from the gpu to the cpu generally the data throughput in both directions is ideally high resulting in a multiplier effect on the speed of a specific high use algorithm
gpgpu pipelines may improve efficiency on especially large data sets and or data containing d or d imagery
it is used in complex graphics pipelines as well as scientific computing more so in fields with large data sets like genome mapping or where two or three dimensional analysis is useful especially at present biomolecule analysis protein study and other complex organic chemistry
such pipelines can also vastly improve efficiency in image processing and computer vision among other fields as well as parallel processing generally
some very heavily optimized pipelines have yielded speed increases of several hundred times the original cpu based pipeline on one high use task
a simple example would be a gpu program that collects data about average lighting values as it renders some view from either a camera or a computer graphics program back to the main program on the cpu so that the cpu can then make adjustments to the overall screen view
a more advanced example might use edge detection to return both numerical information and a processed image representing outlines to a computer vision program controlling say a mobile robot
because the gpu has fast and local hardware access to every pixel or other picture element in an image it can analyze and average it for the first example or apply a sobel edge filter or other convolution filter for the second with much greater speed than a cpu which typically must access slower random access memory copies of the graphic in question
gpgpu is fundamentally a software concept not a hardware concept it is a type of algorithm not a piece of equipment
specialized equipment designs may however even further enhance the efficiency of gpgpu pipelines which traditionally perform relatively few algorithms on very large amounts of data
massively parallelized gigantic data level tasks thus may be parallelized even further via specialized setups such as rack computing many similar highly tailored machines built into a rack which adds a third layer many computing units each using many cpus to correspond to many gpus
some bitcoin miners used such setups for high quantity processing
historically cpus have used hardware managed caches but the earlier gpus only provided software managed local memories
however as gpus are being increasingly used for general purpose applications state of the art gpus are being designed with hardware managed multi level caches which have helped the gpus to move towards mainstream computing
for example geforce series gt architecture gpus did not feature an l cache the fermi gpu has kib last level cache the kepler gpu has mib last level cache the maxwell gpu has mib last level cache and the pascal gpu has mib last level cache
gpus have very large register files which allow them to reduce context switching latency
register file size is also increasing over different gpu generations e g the total register file size on maxwell gm pascal and volta gpus are mib mib and mib respectively
by comparison the size of a register file on cpus is small typically tens or hundreds of kilobytes
the high performance of gpus comes at the cost of high power consumption which under full load is in fact as much power as the rest of the pc system combined
the maximum power consumption of the pascal series gpu tesla p was specified to be w
gpus are designed specifically for graphics and thus are very restrictive in operations and programming
due to their design gpus are only effective for problems that can be solved using stream processing and the hardware can only be used in certain ways
the following discussion referring to vertices fragments and textures concerns mainly the legacy model of gpgpu programming where graphics apis opengl or directx were used to perform general purpose computation
with the introduction of the cuda nvidia and opencl vendor independent general purpose computing apis in new gpgpu codes it is no longer necessary to map the computation to graphics primitives
the stream processing nature of gpus remains valid regardless of the apis used
see e g
gpus can only process independent vertices and fragments but can process many of them in parallel
this is especially effective when the programmer wants to process many vertices or fragments in the same way
in this sense gpus are stream processors processors that can operate in parallel by running one kernel on many records in a stream at once
a stream is simply a set of records that require similar computation
streams provide data parallelism
kernels are the functions that are applied to each element in the stream
in the gpus vertices and fragments are the elements in streams and vertex and fragment shaders are the kernels to be run on them
for each element we can only read from the input perform operations on it and write to the output
it is permissible to have multiple inputs and multiple outputs but never a piece of memory that is both readable and writable
arithmetic intensity is defined as the number of operations performed per word of memory transferred
it is important for gpgpu applications to have high arithmetic intensity else the memory access latency will limit computational speedup
ideal gpgpu applications have large data sets high parallelism and minimal dependency between data elements
there are a variety of computational resources available on the gpu
programmable processors vertex primitive fragment and mainly compute pipelines allow programmer to perform kernel on streams of data
rasterizer creates fragments and interpolates per vertex constants such as texture coordinates and color
texture unit read only memory interface
framebuffer write only memory interfacein fact a program can substitute a write only texture for output instead of the framebuffer
this is done either through render to texture rtt render to backbuffer copy to texture rtbctt or the more recent stream out
the most common form for a stream to take in gpgpu is a d grid because this fits naturally with the rendering model built into gpus
many computations naturally map into grids matrix algebra image processing physically based simulation and so on
since textures are used as memory texture lookups are then used as memory reads
certain operations can be done automatically by the gpu because of this
compute kernels can be thought of as the body of loops
for example a programmer operating on a grid on the cpu might have code that looks like this
on the gpu the programmer only specifies the body of the loop as the kernel and what data to loop over by invoking geometry processing
in sequential code it is possible to control the flow of the program using if then else statements and various forms of loops
such flow control structures have only recently been added to gpus
conditional writes could be performed using a properly crafted series of arithmetic bit operations but looping and conditional branching were not possible
recent gpus allow branching but usually with a performance penalty
branching should generally be avoided in inner loops whether in cpu or gpu code and various methods such as static branch resolution pre computation predication loop splitting and z cull can be used to achieve branching when hardware support does not exist
the map operation simply applies the given function the kernel to every element in the stream
a simple example is multiplying each value in the stream by a constant increasing the brightness of an image
the map operation is simple to implement on the gpu
the programmer generates a fragment for each pixel on screen and applies a fragment program to each one
the result stream of the same size is stored in the output buffer
some computations require calculating a smaller stream possibly a stream of only one element from a larger stream
this is called a reduction of the stream
generally a reduction can be performed in multiple steps
the results from the prior step are used as the input for the current step and the range over which the operation is applied is reduced until only one stream element remains
stream filtering is essentially a non uniform reduction
filtering involves removing items from the stream based on some criteria
the scan operation also termed parallel prefix sum takes in a vector stream of data elements and an arbitrary associative binary function with an identity element i
if the input is a a a a an exclusive scan produces the output i a a a a a a while an inclusive scan produces the output a a a a a a a a a a and does not require an identity to exist
while at first glance the operation may seem inherently serial efficient parallel scan algorithms are possible and have been implemented on graphics processing units
the scan operation has uses in e g quicksort and sparse matrix vector multiplication
the scatter operation is most naturally defined on the vertex processor
the vertex processor is able to adjust the position of the vertex which allows the programmer to control where information is deposited on the grid
other extensions are also possible such as controlling how large an area the vertex affects
the fragment processor cannot perform a direct scatter operation because the location of each fragment on the grid is fixed at the time of the fragment s creation and cannot be altered by the programmer
however a logical scatter operation may sometimes be recast or implemented with another gather step
a scatter implementation would first emit both an output value and an output address
an immediately following gather operation uses address comparisons to see whether the output value maps to the current output slot
in dedicated compute kernels scatter can be performed by indexed writes
gather is the reverse of scatter
after scatter reorders elements according to a map gather can restore the order of the elements according to the map scatter used
in dedicated compute kernels gather may be performed by indexed reads
in other shaders it is performed with texture lookups
the sort operation transforms an unordered set of elements into an ordered set of elements
the most common implementation on gpus is using radix sort for integer and floating point data and coarse grained merge sort and fine grained sorting networks for general comparable data
the search operation allows the programmer to find a given element within the stream or possibly find neighbors of a specified element
the gpu is not used to speed up the search for an individual element but instead is used to run multiple searches in parallel
mostly the search method used is binary search on sorted elements
a variety of data structures can be represented on the gpu
dense arrays
sparse matrices sparse array static or dynamic
adaptive structures union type the following are some of the areas where gpus have been used for general purpose computing
automatic parallelization
physical based simulation and physics engines usually based on newtonian physics models
conway s game of life cloth simulation fluid incompressible flow by solution of euler equations fluid dynamics or navier stokes equations
statistical physics
ising model
lattice gauge theory
segmentation d and d
level set methods
ct reconstruction
fast fourier transform
gpu learning machine learning and data mining computations e g with software bidmach
k nearest neighbor algorithm
fuzzy logic
tone mapping
audio signal processing
audio and sound effects processing to use a gpu for digital signal processing dsp
analog signal processing
speech processing
digital image processing
video processing
hardware accelerated video decoding and post processing
motion compensation mo comp
inverse discrete cosine transform idct
variable length decoding vld huffman coding
inverse quantization iq not to be confused by intelligence quotient
in loop deblocking
bitstream processing cavlc cabac using special purpose hardware for this task because this is a serial task not suitable for regular gpgpu computation
deinterlacing
spatial temporal deinterlacing
noise reduction
edge enhancement
color correction
hardware accelerated video encoding and pre processing
global illumination ray tracing photon mapping radiosity among others subsurface scattering
geometric computing constructive solid geometry distance fields collision detection transparency computation shadow generation
scientific computing
monte carlo simulation of light propagation
weather forecasting
climate research
molecular modeling on gpu
quantum mechanical physics
astrophysics
bioinformatics
computational finance
medical imaging
clinical decision support system cdss
computer vision
digital signal processing signal processing
control engineering
operations research
implementations of the gpu tabu search algorithm solving the resource constrained project scheduling problem is freely available on github the gpu algorithm solving the nurse rerostering problem is freely available on github
neural networks
database operations
computational fluid dynamics especially using lattice boltzmann methods
cryptography and cryptanalysis
performance modeling computationally intensive tasks on gpu
implementations of md advanced encryption standard aes data encryption standard des rsa elliptic curve cryptography ecc
password cracking
cryptocurrency transactions processing mining bitcoin mining
electronic design automation
antivirus software
intrusion detection
increase computing power for distributed computing projects like seti home einstein homegpgpu usage in bioinformatics
expected speedups are highly dependent on system configuration
gpu performance compared against multi core x cpu socket
gpu performance benchmarked on gpu supported features and may be a kernel to kernel performance comparison
for details on configuration used view application website
speedups as per nvidia in house testing or isv s documentation
q quadro gpu t tesla gpu
nvidia recommended gpus for this application
check with developer or isv to obtain certification information
fastra ii
physics engine
advanced simulation library
physics processing unit ppu
close to metal
audio processing unit
larrabee microarchitecture
ai accelerator
deep learning processor dlp


in information technology a backup or data backup is a copy of computer data taken and stored elsewhere so that it may be used to restore the original after a data loss event
the verb form referring to the process of doing so is back up whereas the noun and adjective form is backup
backups can be used to recover data after its loss from data deletion or corruption or to recover data from an earlier time backups provide a simple form of disaster recovery however not all backup systems are able to reconstitute a computer system or other complex configuration such as a computer cluster active directory server or database server
a backup system contains at least one copy of all data considered worth saving
the data storage requirements can be large
an information repository model may be used to provide structure to this storage
there are different types of data storage devices used for copying backups of data that is already in secondary storage onto archive files
there are also different ways these devices can be arranged to provide geographic dispersion data security and portability
data is selected extracted and manipulated for storage
the process can include methods for dealing with live data including open files as well as compression encryption and de duplication
additional techniques apply to enterprise client server backup
backup schemes may include dry runs that validate the reliability of the data being backed up
there are limitations and human factors involved in any backup scheme
a backup strategy requires an information repository a secondary storage space for data that aggregates backups of data sources
the repository could be as simple as a list of all backup media dvds etc and the dates produced or could include a computerized index catalog or relational database
the backup data needs to be stored requiring a backup rotation scheme which is a system of backing up data to computer media that limits the number of backups of different dates retained separately by appropriate re use of the data storage media by overwriting of backups no longer needed
the scheme determines how and when each piece of removable storage is used for a backup operation and how long it is retained once it has backup data stored on it
the rule can aid in the backup process
it states that there should be at least copies of the data stored on different types of storage media and one copy should be kept offsite in a remote location this can include cloud storage
or more different media should be used to eliminate data loss due to similar reasons for example optical discs may tolerate being underwater while lto tapes may not and ssds cannot fail due to head crashes or damaged spindle motors since they don t have any moving parts unlike hard drives an offsite copy protects against fire theft of physical media such as tapes or discs and natural disasters like floods and earthquakes
disaster protected hard drives like those made by iosafe are an alternative to an offsite copy but they have limitations like only being able to resist fire for a limited period of time so an offsite copy still remains as the ideal choice
an unstructured repository may simply be a stack of tapes dvd rs or external hdds with minimal information about what was backed up and when
this method is the easiest to implement but unlikely to achieve a high level of recoverability as it lacks automation
a repository using this backup method contains complete source data copies taken at one or more specific points in time
copying system images this method is frequently used by computer technicians to record known good configurations
however imaging is generally more useful as a way of deploying a standard configuration to many systems rather than as a tool for making ongoing backups of diverse systems
an incremental backup stores data changed since a reference point in time
duplicate copies of unchanged data aren t copied
typically a full backup of all files is once or at infrequent intervals serving as the reference point for an incremental repository
subsequently a number of incremental backups are made after successive time periods
restores begin with the last full backup and then apply the incrementals
some backup systems can create a synthetic full backup from a series of incrementals thus providing the equivalent of frequently doing a full backup
when done to modify a single archive file this speeds restores of recent versions of files
continuous data protection cdp refers to a backup that instantly saves a copy of every change made to the data
this allows restoration of data to any point in time and is the most comprehensive and advanced data protection
near cdp backup applications often marketed as cdp automatically take incremental backups at a specific interval for example every minutes one hour or hours
they can therefore only allow restores to an interval boundary
near cdp backup applications use journaling and are typically based on periodic snapshots read only copies of the data frozen at a particular point in time
near cdp except for apple time machine intent logs every change on the host system often by saving byte or block level differences rather than file level differences
this backup method differs from simple disk mirroring in that it enables a roll back of the log and thus a restoration of old images of data
intent logging allows precautions for the consistency of live data protecting self consistent files but requiring applications be quiesced and made ready for backup
near cdp is more practicable for ordinary personal backup applications as opposed to true cdp which must be run in conjunction with a virtual machine or equivalent and is therefore generally used in enterprise client server backups
a reverse incremental backup method stores a recent archive file mirror of the source data and a series of differences between the mirror in its current state and its previous states
a reverse incremental backup method starts with a non image full backup
after the full backup is performed the system periodically synchronizes the full backup with the live copy while storing the data necessary to reconstruct older versions
this can either be done using hard links as apple time machine does or using binary diffs
a differential backup saves only the data that has changed since the last full backup
this means a maximum of two backups from the repository are used to restore the data
however as time from the last full backup and thus the accumulated changes in data increases so does the time to perform the differential backup
restoring an entire system requires starting from the most recent full backup and then applying just the last differential backup
a differential backup copies files that have been created or changed since the last full backup regardless of whether any other differential backups have been made since whereas an incremental backup copies files that have been created or changed since the most recent backup of any type full or incremental
changes in files may be detected through a more recent date time of last modification file attribute and or changes in file size
other variations of incremental backup include multi level incrementals and block level incrementals that compare parts of files instead of just entire files
regardless of the repository model that is used the data has to be copied onto an archive file data storage medium
the medium used is also referred to as the type of backup destination
magnetic tape was for a long time the most commonly used medium for bulk data storage backup archiving and interchange
it was previously a less expensive option but this is no longer the case for smaller amounts of data
tape is a sequential access medium so the rate of continuously writing or reading data can be very fast
while tape media itself has a low cost per space tape drives are typically dozens of times as expensive as hard disk drives and optical drives
many tape formats have been proprietary or specific to certain markets like mainframes or a particular brand of personal computer
by lto had become the primary tape technology
the other remaining viable super format is the ibm also referred to as the ts xx series
the oracle storagetek t was discontinued in
the use of hard disk storage has increased over time as it has become progressively cheaper
hard disks are usually easy to use widely available and can be accessed quickly
however hard disk backups are close tolerance mechanical devices and may be more easily damaged than tapes especially while being transported
in the mid s several drive manufacturers began to produce portable drives employing ramp loading and accelerometer technology sometimes termed a shock sensor and by the industry average in drop tests for drives with that technology showed drives remaining intact and working after a inch non operating drop onto industrial carpeting
some manufacturers also offer ruggedized portable hard drives which include a shock absorbing case around the hard disk and claim a range of higher drop specifications
over a period of years the stability of hard disk backups is shorter than that of tape backups
external hard disks can be connected via local interfaces like scsi usb firewire or esata or via longer distance technologies like ethernet iscsi or fibre channel
some disk based backup systems via virtual tape libraries or otherwise support data deduplication which can reduce the amount of disk storage capacity consumed by daily and weekly backup data
optical storage uses lasers to store and retrieve data
recordable cds dvds and blu ray discs are commonly used with personal computers and are generally cheap
in the past the capacities and speeds of these discs have been lower than hard disks or tapes although advances in optical media are slowly shrinking that gap
potential future data losses caused by gradual media degradation can be predicted by measuring the rate of correctable minor data errors of which consecutively too many increase the risk of uncorrectable sectors
support for error scanning varies among optical drive vendors
many optical disc formats are worm type which makes them useful for archival purposes since the data cannot be changed
moreover optical discs are not vulnerable to head crashes magnetism imminent water ingress or power surges and a fault of the drive typically just halts the spinning
optical media is modular the storage controller is not tied to media itself like with hard drives or flash storage flash memory controller allowing it to be removed and accessed through a different drive
however recordable media may degrade earlier under long term exposure to light
some optical storage systems allow for cataloged data backups without human contact with the discs allowing for longer data integrity
a french study in indicated that the lifespan of typically sold cd rs was years but one manufacturer later estimated the longevity of its cd rs with a gold sputtered layer to be as high as years
sony s proprietary optical disc archive can in reach a read rate of mb s
solid state drives ssds use integrated circuit assemblies to store data
flash memory thumb drives usb flash drives compactflash smartmedia memory sticks and secure digital card devices are relatively expensive for their low capacity but convenient for backing up relatively low data volumes
a solid state drive does not contain any movable parts making it less susceptible to physical damage and can have huge throughput of around mbit s up to gbit s
available ssds have become more capacious and cheaper
flash memory backups are stable for fewer years than hard disk backups
remote backup services or cloud backups involve service providers storing data offsite
this has been used to protect against events such as fires floods or earthquakes which could destroy locally stored backups
cloud based backup through services like or similar to google drive and microsoft onedrive provides a layer of data protection
however the users must trust the provider to maintain the privacy and integrity of their data with confidentiality enhanced by the use of encryption
because speed and availability are limited by a user s online connection users with large amounts of data may need to use cloud seeding and large scale recovery
various methods can be used to manage backup media striking a balance between accessibility security and cost
these media management methods are not mutually exclusive and are frequently combined to meet the user s needs
using on line disks for staging data before it is sent to a near line tape library is a common example
online backup storage is typically the most accessible type of data storage and can begin a restore in milliseconds
an internal hard disk or a disk array maybe connected to san is an example of an online backup
this type of storage is convenient and speedy but is vulnerable to being deleted or overwritten either by accident by malevolent action or in the wake of a data deleting virus payload
nearline storage is typically less accessible and less expensive than online storage but still useful for backup data storage
a mechanical device is usually used to move media units from storage into a drive where the data can be read or written
generally it has safety properties similar to on line storage
an example is a tape library with restore times ranging from seconds to a few minutes
off line storage requires some direct action to provide access to the storage media for example inserting a tape into a tape drive or plugging in a cable
because the data is not accessible via any computer except during limited periods in which they are written or read back they are largely immune to on line backup failure modes
access time varies depending on whether the media are on site or off site
backup media may be sent to an off site vault to protect against a disaster or other site specific problem
the vault can be as simple as a system administrator s home office or as sophisticated as a disaster hardened temperature controlled high security bunker with facilities for backup media storage
a data replica can be off site but also on line e g an off site raid mirror
such a replica has fairly limited value as a backup
a backup site or disaster recovery center is used to store data that can enable computer systems and networks to be restored and properly configure in the event of a disaster
some organisations have their own data recovery centres while others contract this out to a third party
due to high costs backing up is rarely considered the preferred method of moving data to a dr site
a more typical way would be remote disk mirroring which keeps the dr data as up to date as possible
a backup operation starts with selecting and extracting coherent units of data
most data on modern computer systems is stored in discrete units known as files
these files are organized into filesystems
deciding what to back up at any given time involves tradeoffs
by backing up too much redundant data the information repository will fill up too quickly
backing up an insufficient amount of data can eventually lead to the loss of critical information
copying files making copies of files is the simplest and most common way to perform a backup
a means to perform this basic function is included in all backup software and all operating systems
partial file copying a backup may include only the blocks or bytes within a file that have changed in a given period of time
this can substantially reduce needed storage space but requires higher sophistication to reconstruct files in a restore situation
some implementations require integration with the source file system
deleted files to prevent the unintentional restoration of files that have been intentionally deleted a record of the deletion must be kept
versioning of files most backup applications other than those that do only full only system imaging also back up files that have been modified since the last backup
that way you can retrieve many different versions of a given file and if you delete it on your hard disk you can still find it in your information repository archive filesystem dump a copy of the whole filesystem in block level can be made
this is also known as a raw partition backup and is related to disk imaging
the process usually involves unmounting the filesystem and running a program like dd unix
because the disk is read sequentially and with large buffers this type of backup can be faster than reading every file normally especially when the filesystem contains many small files is highly fragmented or is nearly full
but because this method also reads the free disk blocks that contain no useful data this method can also be slower than conventional reading especially when the filesystem is nearly empty
some filesystems such as xfs provide a dump utility that reads the disk sequentially for high performance while skipping unused sections
the corresponding restore utility can selectively restore individual files or the entire volume at the operator s choice
identification of changes some filesystems have an archive bit for each file that says it was recently changed
some backup software looks at the date of the file and compares it with the last backup to determine whether the file was changed
versioning file system a versioning filesystem tracks all changes to a file
the nilfs versioning filesystem for linux is an example files that are actively being updated present a challenge to back up
one way to back up live data is to temporarily quiesce them e g close all files take a snapshot and then resume live operations
at this point the snapshot can be backed up through normal methods
a snapshot is an instantaneous function of some filesystems that presents a copy of the filesystem as if it were frozen at a specific point in time often by a copy on write mechanism
snapshotting a file while it is being changed results in a corrupted file that is unusable
this is also the case across interrelated files as may be found in a conventional database or in applications such as microsoft exchange server
the term fuzzy backup can be used to describe a backup of live data that looks like it ran correctly but does not represent the state of the data at a single point in time
backup options for data files that cannot be or are not quiesced include
open file backup many backup software applications undertake to back up open files in an internally consistent state
some applications simply check whether open files are in use and try again later
other applications exclude open files that are updated very frequently
some low availability interactive applications can be backed up via natural induced pausing
interrelated database files backup some interrelated database file systems offer a means to generate a hot backup of the database while it is online and usable
this may include a snapshot of the data files plus a snapshotted log of changes made while the backup is running
upon a restore the changes in the log files are applied to bring the copy of the database up to the point in time at which the initial backup ended
other low availability interactive applications can be backed up via coordinated snapshots
however genuinely high availability interactive applications can be only be backed up via continuous data protection not all information stored on the computer is stored in files
accurately recovering a complete system from scratch requires keeping track of this non file data too
system description system specifications are needed to procure an exact replacement after a disaster
boot sector the boot sector can sometimes be recreated more easily than saving it
it usually isn t a normal file and the system won t boot without it
partition layout the layout of the original disk as well as partition tables and filesystem settings is needed to properly recreate the original system
file metadata each file s permissions owner group acls and any other metadata need to be backed up for a restore to properly recreate the original environment
system metadata different operating systems have different ways of storing configuration information
microsoft windows keeps a registry of system information that is more difficult to restore than a typical file it is frequently useful or required to manipulate the data being backed up to optimize the backup process
these manipulations can improve backup speed restore speed data security media usage and or reduced bandwidth requirements
out of date data can be automatically deleted but for personal backup applications as opposed to enterprise client server backup applications where automated data grooming can be customized the deletion can at most be globally delayed or be disabled
various schemes can be employed to shrink the size of the source data to be stored so that it uses less storage space
compression is frequently a built in feature of tape drive hardware
redundancy due to backing up similarly configured workstations can be reduced thus storing just one copy
this technique can be applied at the file or raw block level
this potentially large reduction is called deduplication
it can occur on a server before any data moves to backup media sometimes referred to as source client side deduplication
this approach also reduces bandwidth required to send backup data to its target media
the process can also occur at the target storage device sometimes referred to as inline or back end deduplication
sometimes backups are duplicated to a second set of storage media
this can be done to rearrange the archive files to optimize restore speed or to have a second copy at a different location or on a different storage medium as in the disk to disk to tape capability of enterprise client server backup
high capacity removable storage media such as backup tapes present a data security risk if they are lost or stolen
encrypting the data on these media can mitigate this problem however encryption is a cpu intensive process that can slow down backup speeds and the security of the encrypted backups is only as effective as the security of the key management policy
when there are many more computers to be backed up than there are destination storage devices the ability to use a single storage device with several simultaneous backups can be useful
however cramming the scheduled backup window via multiplexed backup is only used for tape destinations
the process of rearranging the sets of backups in an archive file is known as refactoring
for example if a backup system uses a single tape each day to store the incremental backups for all the protected computers restoring one of the computers could require many tapes
refactoring could be used to consolidate all the backups for a single computer onto a single tape creating a synthetic full backup
this is especially useful for backup systems that do incrementals forever style backups
sometimes backups are copied to a staging disk before being copied to tape
this process is sometimes referred to as d d t an acronym for disk to disk to tape
it can be useful if there is a problem matching the speed of the final destination device with the source device as is frequently faced in network based backup systems
it can also serve as a centralized location for applying other data manipulation techniques
recovery point objective rpo the point in time that the restarted infrastructure will reflect expressed as the maximum targeted period in which data transactions might be lost from an it service due to a major incident
essentially this is the roll back that will be experienced as a result of the recovery
the most desirable rpo would be the point just prior to the data loss event
making a more recent recovery point achievable requires increasing the frequency of synchronization between the source data and the backup repository
recovery time objective rto the amount of time elapsed between disaster and restoration of business functions
data security in addition to preserving access to data for its owners data must be restricted from unauthorized access
backups must be performed in a manner that does not compromise the original owner s undertaking
this can be achieved with data encryption and proper media handling policies
data retention period regulations and policy can lead to situations where backups are expected to be retained for a particular period but not any further
retaining backups after this period can lead to unwanted liability and sub optimal use of storage media
checksum or hash function validation applications that back up to tape archive files need this option to verify that the data was accurately copied
backup process monitoring enterprise client server backup applications need a user interface that allows administrators to monitor the backup process and proves compliance to regulatory bodies outside the organization for example an insurance company in the usa might be required under hipaa to demonstrate that its client data meet records retention requirements
user initiated backups and restores to avoid or recover from minor disasters such as inadvertently deleting or overwriting the good versions of one or more files the computer user rather than an administrator may initiate backups and restores from not necessarily the most recent backup of files or folders about backupbackup software services
list of backup software
list of online backup services
glossary of backup terms
virtual backup appliancerelated topicsdata consistency
data degradation
data portability
data proliferation
database dump
digital preservation
disaster recovery and business continuity auditing the dictionary definition of backup at wiktionary
media related to backup at wikimedia commons

